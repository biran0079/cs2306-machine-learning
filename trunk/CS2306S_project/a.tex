\documentclass[a4paper,11pt]{article}
\usepackage{amssymb}
\usepackage{mathbbold}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{url}
\usepackage{amsthm}
\parindent=0in
\author{Bi Ran(U087272L)}
\title{Hand Written Digit Recognition and Its Improvement}
\frenchspacing

\begin{document}
\maketitle
\begin{abstract}
Handwritten digits recognition is classic problem of machine learning. The objective is to recognize images of single handwritten digits(0-9). In this paper, we apply C4.5 decision tree algorithm, boosting algorithm on C4.5, support vector machine(SVM) algorithm on this problem. The later part of the paper gives a way to improvement the classification accuracy by adding a few extra attributes into data set.
\end{abstract}
\section{Introduction}
Handwritten digit recognition recognition is a classification problem. The performance of classification algorithm heavily depends on the feature of data set.
In this paper, Semeion Handwritten Digit Data Set is used for training and testing. The first part of the paper will show the result of different algorithms.
It will be shown that the performance of decision tree is poor by itself, but much better after being boosted; SVM gives the best performance in these algorithms.
The second part will try to improve performance of classifiers by adding extra features into data set. It will be shown that performance of all three algorithms
 has significant improvement on the modified data set, and SVM with 2 degree polynomial kernel achieves error rate 3.7037\%, which is the lowest of all three algorithms.
\section{Background and Motivation}
Handwritten digit recognition has important usage such as recognizing zip code, online recognition on computer tablets. Algorithms like support vector machine and multi layer perceptron are popular in this field. Currently, Lenet and its boosted version achieves error rate less than 1\% on MNIST data set. SVM also gives one of the lowest error rate among many classification algorithms.
\section{Data set}
Semeion Handwritten Digit Data Set is used in this paper. The data set was created by Tactile Srl, Brescia, Italy (\url{http://www.tattile.it}) and donated in 1994 to Semeion Research Center of Sciences of Communication, Rome, Italy (\url{http://www.semeion.it}), for machine learning research.
1593 handwritten digits from around 80 persons were scanned, stretched in a rectangular box 16x16 in a gray scale of 256 values.Then each pixel of each image was scaled into a boolean (1/0) value using a fixed threshold. This data set contains no missing values.
Each person wrote on a paper all the digits from 0 to 9, twice. The commitment was to write the digit the first time in the normal way (trying to write each digit accurately) and the second time in a fast way (with no accuracy).

This data set consists of 1593 records (rows) and 256 attributes (columns).
Each record represents a handwritten digit, originally scanned with a resolution of 256 grays scale.
Each pixel of the each original scanned image was first stretched, and after scaled between 0 and 1 (setting to 0 every pixel whose value was under the value 127 of the grey scale (127 included) and setting to 1 each pixel whose original value in the grey scale was over 127).
Finally, each binary image was scaled again into a 16x16 square box (the final 256 binary attributes).\\
The order of the examples in the data set is shuffled before training and testing because the original order of the data set is not really random.
\section{Evaluation}

The data set contains 1593 instances, which not very big. In this paper, cross validation is used to perform testing. \\
By applying cross validation on different fold number using C4.5 algorithm, we can see that the error rate almost keeps still after 4 fold. 4 fold cross validation is very likely to provide accurate estimation of the real error rate on unseen data. To be safe, 10 fold cross validation is used in this paper in all cases of testing.

\vspace{0.5cm}
\begin{tabular}{c c}
Fold Number & Error Rate\\
\hline \hline
2  &29.0019\%\\
4  &25.6121\%\\
6  &24.231 \%\\
8  & 24.6704\%\\
10 & 23.9799\%\\
12 & 24.8588\%\\
14 & 24.5449\%\\
\end{tabular}
\vspace{0.5cm}\\
\section{Testing}
\subsection{C4.5}
Decision tree is a popular algorithm in data mining and machine learning. It represents a function in form of a decision tree, each node denote an attribute, and each edge represent the condition which lead to the next node. Leaves represent labels, which are the result of classification. Decision tree is efficient and simple. The result tree can be visualized, and pretty human readable. There are many decision tree algorithms, and we only use C4.5 in this paper, which is an improved version of ID3 algorithm. C4.5 algorithm follows a simple inductive bias, which is "smaller trees are preferred", which means that smaller trees tend to perform better than bigger trees on unseen data. This inductive bias is motivated by Occan's Razor principle, the primal principle in machine learning. A free implementation of C4.5 algorithm in java is available in J48 package.\\
The result in the table above shows that he error rate of C4.5 algorithm is between 24\% and 25\%. Adjusting parameters C4.5 does not give significant improvement.
This result is very poor compared to many other popular algorithms like support vector machine(will see it later). This is because the limitation of decision tree itself. Although a decision tree with no size limit can be proved to form any function, C4.5 algorithm prefers smaller trees,
which potentially reduce the number candidate hypothesis. So sometimes the result decision tree may not have a very high accuracy.\\

\subsection{Boosting}
Boosting is an algorithm which aims to boost week classifier into a stronger classifier. The boosting algorithm used here is AdaBoosting M1, which is
generalization of AdaBoosting algorithm on multiple class classification problem. It requires over 50\% accuracy of "week classifier" on the data set
in order to ``boost'' it into a stronger classifier.\\
The boosting algorithm constructs the classifier by several iterations, each iteration obtains a classifier by a weighted data set, and the output of the final classifier is obtained by weighted sum of the results of all classifiers. After each iteration, data set is reweighted, and misclassified instances has a larger weight so that they are paid more attention on in the next iteration. The weight of each classifier is decided by their accuracy. Boosting works well with decision tree in practice, and it often does not suffer from over fitting problem.\\
Following result is obtained by applying boosting on C4.5 algorithm with different number of iterations.

\vspace{0.5cm}
\begin{tabular}{c c}
Number of Iterations	& Error rate\\
\hline \hline
	2		& 24.3566\%\\
	4		& 17.1375\%\\
	8		& 12.6177\%\\
	16		& 8.7884\%\\
	32		& 7.5957\%\\
	64		& 7.2819\%\\
\end{tabular}
\vspace{0.5cm}\\
Boosting algorithm constructs a classifier by combining several classifiers, which increase the complexity of the classifier. However, according to Occan's Razor principle, simpler classifiers tend to generalize better on unseen data. Boosting algorithm seems a contradict with Occan's Razor principle.
One possible explanation is that the complexity of classifier is not measured by the number of classifiers, but by the "margin" of the classifier.
In Boosting algorithm, each classifier vote to a class by its weight, and the class with the highest weight is the result of classification. The "margin" of an example is defined by the difference between weight of correct class, and the weight of the sum of all weights of incorrect class. The larger the margin is , the more confident the classification for this instance is. The boosting algorithm tend to maximize margin of instances, and convergence with some large margin distribution. Sometimes we can observe that when running boosting algorithm, the testing error keeps on decreases even after the error rate of training set is 0. This is because when the error rate of testing data is zero, the margin distribution may not have converged yet, and as margin increases, the error rate on testing data set decreases.
\subsection{Support Vector Machine}
Support vector machine can be seen as a generalized version of linear classifier. To classify data set which are not linear separatable, SVM project each example into a higher dimensional feature space, and find a hyper plane, which separate different classes. The margin of hyper plane is defined as the distance of the plane to the closest example. Hyper plane with large margin is preferred in SVM. All calculation of feature vector with high dimension can be done by kernel function, which reduce the computation time dramatically. Different kernel function can be used for different feature space. In this paper, we only use polynomial kernels($(\vec{u}*\vec{v})^{degree}$).

One SVM can classify two classes. For multi class classification, two approaches are commonly used: one versus rest and pair-wise approach. For handwritten digit recognition problem, one versus rest approach construct 10 SVMs, one for each digit $i$, which classify ``the input digit is $i$ or not''.  The output of SVM is real number, so the digit with the highest output is classified as the class of the input. Pair-wise approach construct 45 SVMs, one for each pair of digits $i$ and $j$, which determines the input digit is $i$ or $j$. Each SVM vote for one digit, and the digit with the highest vote is the result. One problem for one versus rest is that the class of training set is not balance, which may reduce the accuracy. Usually, pair-wise method has a better performance. In this paper, pair-wise approach is used.\\
There are many kinds of SVMs based on different error function it aims to minimize. In this paper, we simply use C-svm, which minimize function:
 subject to:$$\frac{1}{2}\|\vec{\beta}\|^2+C\sum_{i=1}^n \xi_i$$
 subject to:$$y_i(\vec{\beta}.\vec{x_i}+\beta{0})\geq 1-\xi_i$$$$\forall i, \xi_i\geq 0$$
\emph{libsvm} package(\url{http://www.csie.ntu.edu.tw/~cjlin/libsvm/}) implemented by Chih-Chung Chang and Chih-Jen Lin is used for training and testing purpose in this paper.

\vspace{0.5cm}
\begin{tabular}{c c}
Kernel		&	Error rate\\
\hline \hline
polynomial degree 1	&7.0935 \%\\
polynomial degree 2	&4.2687 \%\\
polynomial degree 3	&4.6453 \%\\
polynomial degree 4	&6.1519 \%\\
polynomial degree 5	&8.349  \%\\
\end{tabular}
\vspace{0.5cm}\\
The result shows that SVM gives very good result. We can observe that the error rate first increases, and then decreases as the degree of the polynomial kernel increases. This is because the hyper plain tends to be more flexible to fit the training set when the degree is larger, which can result in over fitting, especially when the data set is limited.

\section{Improvement}
\subsection{Motivation}
One observation is that for decision tree and SVM, the order of attributes is not important for training and classification. In other word, if I apply some fixed permutation on the attributes of each example in the dataset, then decision tree and SVM will generate almost the same model as the model generated by original data set.
However, the order of pixel is important in the process of human being recognizing a hand written digit. If the rearrange the pixel of an image randomly, a human is very unlikely to recognize what's in the image.
Moreover, some properties of the data set are harder to learn for certain classifiers(e.g. decision tree are not good at representing XOR function).
This motivate the preprocessing of data. More specifically, we try to add some attributes which provide those information which are hard to learn by machine learning algorithms.
\subsection{Adding Features}
34 extra features are added into each examples.The modified data set has 290 features for each example.
To provide information about the position of pixels, we choose to add 16 extra attributes $r[16]$, one for each row, which describes how many ``black segments'' are there in the row. In image of 1, most of $r[i]$ can be expected to be 1, and some can be 2, depends on the way of writing. In image of 0, most $r[i]$ can be expected to be 2.
Similarly, 16 more attributes $c[16]$ is added for each column, which means the number of `black segments'' in each column.

The variance of the number of black pixels in each row, and the variance of the number of black pixels in each column are also added into each example.
\subsection{Testing}
\vspace{0.5cm}
\begin{tabular}{c c c}
Algorithm		&	Original Error Rate	Error &Rate On Blur Digit\\
\hline \hline
C4.5		&		23.9799 \%		&18.6441 \%\\
Boosted C4.5(8 iterations)	&12.6177 \%		& 7.8468 \%\\
Boosted C4.5(16 iterations)	&8.7884 \%		 6.5286 \%\\
SVM(polynomial d-1 kernel)	&7.0935 \%		 4.2687 \%\\
SVM(polynomial d-2 kernel)	&4.2687 \%		 3.7037 \%\\
SVM(polynomial d-3 kernel)	&4.6453 \%		 3.892  \%\\
SVM(polynomial d-4 kernel)	&6.1519 \%		 5.022  \%\\
\end{tabular}
\vspace{0.5cm}\\
From the result, we can see improvements on all algorithms are significant. 34 attributes is a small number compared to original 256 attributes, so the increase in time cost is trivial. So adding extra information tend to help improve accuracy for these algorithms.
\section{Conclusion}
By applying several machine learning algorithms on Semeion Handwritten Digit Data Set, we can observe that C4.5 algorithm performs poorly, while boosted C4.5 has a much better performance. SVM performs very well with linear kernel, and even better with polynomial kernel of degree 2 and 3. By adding extra information into the data set, the error rate for all these algorithm drops significantly, and SVM with polynomial d-2 kernel has the lowest error rate.
\section{References}
\end{document}
