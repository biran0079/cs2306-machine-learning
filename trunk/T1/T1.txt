1. Convert dataset into WEKA's arrf format.
	1.1 MNIST dataset
The dataset itself is stored in binary file for the purpose of compression. To make modification of dataset easier in the future, fitst comvert it into fuman readable text file.
MNISTDataToTxt.cpp do this job. For more detail, please refer to the comment of source file.

Then convert text file into WEKA's arff file.
MNISTTxtToArff.cpp do this job. For more detail, please refer to the comment of source file.

	1.2 Semeion dataset
This dataset itself is in human readable format. So just need one step to convert it into arff fotmat.
2. C4.5 algorithm
Use J48 pachage, a C4.5 algorithm implementated in java. 

MNIST


		

TRAINING DATA:

confident factor: 0.25
percentage for validation dataset: 30%
Time taken to build model: 352.1 second
Correctly Classified on training set               97.2867%

=== Detailed Accuracy By Class ===

               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                 0.994     0.002      0.978     0.994     0.986      1        0
                 0.987     0.002      0.987     0.987     0.987      1        1
                 0.972     0.004      0.967     0.972     0.969      0.999    2
                 0.961     0.004      0.966     0.961     0.964      0.998    3
                 0.973     0.003      0.976     0.973     0.974      0.999    4
                 0.958     0.004      0.961     0.958     0.96       0.998    5
                 0.982     0.002      0.98      0.982     0.981      0.999    6
                 0.974     0.003      0.972     0.974     0.973      0.999    7
                 0.96      0.004      0.964     0.96      0.962      0.998    8
                 0.964     0.003      0.973     0.964     0.969      0.998    9
Weighted Avg.    0.973     0.003      0.973     0.973     0.973      0.999

2,3,5,8 are harder to clasify.

=== Confusion Matrix ===

    a    b    c    d    e    f    g    h    i    j   <-- classified as
 2943    1    4    0    2    0    5    0    5    1 |    a = 0
    1 3380    4    7    3    3    7   10    8    0 |    b = 1
   17    4 2864   12    5    6    5   22   12    1 |    c = 2
    4    8   29 2954    3   32    4   18   17    4 |    d = 3
    3    4   13    6 2847    5    5    5   13   25 |    e = 4
    9    7   15   21    4 2596   18    4   17   18 |    f = 5
    7    2    8   11    7   12 2921    0    7    0 |    g = 6
    7    6    9   15   10   10    2 3026    5   17 |    h = 7
    9    8    9   19   18   21   12    5 2761   13 |    i = 8
    8    5    8   12   19   15    1   22   19 2894 |    j = 9

Easy to get confused:
2->7
3->2
3->5
4->9
5->3
8->5
9->7

TESTING DATA:

1000 instatnces, each 785 attributes.
Correctly Classified on training set                87.67%

=== Detailed Accuracy By Class ===

               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                 0.953     0.01       0.91      0.953     0.931      0.977    0
                 0.957     0.009      0.929     0.957     0.943      0.979    1
                 0.874     0.017      0.859     0.874     0.866      0.931    2
                 0.835     0.018      0.838     0.835     0.836      0.922    3
                 0.864     0.013      0.877     0.864     0.87       0.927    4
                 0.789     0.016      0.828     0.789     0.808      0.885    5
                 0.907     0.009      0.915     0.907     0.911      0.952    6
                 0.906     0.012      0.893     0.906     0.9        0.956    7
                 0.798     0.018      0.828     0.798     0.813      0.88     8
                 0.865     0.014      0.871     0.865     0.868      0.927    9
Weighted Avg.    0.877     0.014      0.876     0.877     0.876      0.935

Accuracy for each digit are much lower than training dataset.
2,3,4,5,7,8,9 are harder to recognize, especially 3,5,8, similar to training dataset.


=== Confusion Matrix ===

    a    b    c    d    e    f    g    h    i    j   <-- classified as
  934    2    8    4    4    4   11    2    3    8 |    a = 0
    0 1086    6   13    2    4    8    3   13    0 |    b = 1
   15   14  902   28    8    7    8   20   26    4 |    c = 2
    8    6   32  843    7   56    2   24   25    7 |    d = 3
    9   15   15    8  848   13    9    9   16   40 |    e = 4
   12    9   15   54    9  704   19   18   25   27 |    f = 5
   23    5    7    4   11   14  869    5   18    2 |    g = 6
    1    9   28   11   17    2    1  931    9   19 |    h = 7
   18   15   30   22   23   34   23   10  777   22 |    i = 8
    6    8    7   19   38   12    0   20   26  873 |    j = 9

2->3,8,9
3->2,5
4->9
5->8,9
6->0
7->2
8->*
9->4,7,8

Many are similar to the mistakes in the training dataset.
3<->5, 9<->4 are serious problems.


Training accuracy much higher than testing accuracy. Overfitting may exists.

Guess:
Perform more pruning may get better accuracy on testing data.
	1. Reduce confidence factor from 0.25 to 0.1, which result in more pruning. 
		Result: 
			Accuracy on training data: 96.84%, slightly lower.
			Accuracy on testing data:  87.77%, almost the same.
	2. Increase percentage of validation data from 30% to 50%.
		Result:
			Accuracy on training data: 97.2867, no change
			Accuracy on testing data:  87.67% no change
			
			



3. BayesNet algorithm

Accuracy on training data: 84.0033%
Time taken to build model: 60.67 seconds

Faster than decision tree.

TRAINING DATA:

=== Detailed Accuracy By Class ===

               TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                 0.887     0.01       0.91      0.887     0.899      0.993    0
                 0.96      0.016      0.884     0.96      0.92       0.994    1
                 0.826     0.015      0.86      0.826     0.843      0.983    2
                 0.804     0.026      0.78      0.804     0.792      0.975    3
                 0.81      0.018      0.827     0.81      0.818      0.985    4
                 0.731     0.02       0.784     0.731     0.757      0.974    5
                 0.906     0.012      0.891     0.906     0.899      0.992    6
                 0.855     0.006      0.939     0.855     0.895      0.991    7
                 0.761     0.022      0.789     0.761     0.774      0.973    8
                 0.83      0.033      0.737     0.83      0.781      0.975    9
Weighted Avg.    0.84      0.018      0.842     0.84      0.84       0.984

Precision much lower than C4.5 Decicion tree.
3,5,8,9 are hard to classify, similar to decisoin tree.


=== Confusion Matrix ===

    a    b    c    d    e    f    g    h    i    j   <-- classified as
 2627    0   22   12    5  164   61    0   65    5 |    a = 0
    0 3287   33    9    1   41    9    1   39    3 |    b = 1
   44   46 2436   82   56   14  114   29  118    9 |    c = 2
   22   73  136 2472   13  100   22   30  115   90 |    d = 3
    8   21   25    1 2369    7   41    7   64  383 |    e = 4
   79   29   27  328   73 1980   59    6   47   81 |    f = 5
   30   65   57    1   22   92 2696    0   12    0 |    g = 6
    8   57   35    5   86    7    1 2655   50  203 |    h = 7
   35  111   41  222   40  101   20    5 2187  113 |    i = 8
   33   31   19   39  198   18    2   95   76 2492 |    j = 9

0->5
2->6,8
3->2,5,8
4->9
5->3
7->9
8->1,3,5
9->4

TESTING DATA:

Accuracy: 84.84   %
Slightly lower than training data accuracy. Lower than C4.5 algorithm.

